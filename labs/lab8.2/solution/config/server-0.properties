broker.id=0

listeners=SASL_PLAINTEXT://localhost:9092,SASL_SSL://localhost:10092
sasl.mechanism.inter.broker.protocol=GSSAPI
sasl.enabled.mechanisms=GSSAPI
sasl.kerberos.service.name=kafka




ssl.keystore.location=/opt/kafka/conf/certs/kafka.keystore
ssl.keystore.password=kafka123
ssl.key.password=kafka123
ssl.truststore.location=/opt/kafka/conf/certs/kafka.truststore
ssl.truststore.password=kafka123
ssl.client.auth=required


# Kafka should have its own dedicated disk(s) or use SSD(s)
# To increase reads and writes, add more disks/log dirs JBOD.
log.dirs=./logs/kafka-0

## Log config
default.replication.factor=3
num.partitions=8

## Data must be replicated to at least two brokers
min.insync.replicas=2 

## Don't allow un-managed topics for production
auto.create.topics.enable=false

## Run brokers spread over AZs or Racks
broker.rack=us-west2-a

## Number of concurrent requests allowed
queued.max.requests=1000

## Allow leaders to auto rebalance
auto.leader.rebalance.enable=true


zookeeper.connect=localhost:2181
delete.topic.enable=true
compression.type=producer
message.max.bytes=65536
replica.lag.time.max.ms=5000
num.network.threads=3
num.io.threads=8
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
num.recovery.threads.per.data.dir=1
log.retention.hours=168
log.segment.bytes=1073741824
log.retention.check.interval.ms=300000
zookeeper.connection.timeout.ms=6000
